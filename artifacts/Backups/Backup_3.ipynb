{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports and Configurations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "ecm = 'mean_absolute_error'\n",
    "# ecm = 'mean_squared_error'\n",
    "k_fold_value = 5\n",
    "\n",
    "raw_dataset = pd.read_csv('dataset_v1.csv', sep=',', skipinitialspace=True)\n",
    "dataset = raw_dataset.copy()\n",
    "predict_set = dataset.tail(20)\n",
    "dataset = dataset.drop(predict_set.index)\n",
    "\n",
    "predictors = ['SampleNo', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6']\n",
    "all_test_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Utility Classes\n",
    "class ModelResult:\n",
    "    def __init__(self, info_dict: dict):\n",
    "        self.model = info_dict['model']\n",
    "        self.arg = info_dict['arg']\n",
    "        self.predictors = info_dict['predictors']\n",
    "        if 'error' not in info_dict.keys():\n",
    "            self.y_actual = info_dict['y_actual']\n",
    "            self.y_predicted = info_dict['y_predicted']\n",
    "            self.error = ModelResult.calculate_error(y_act=self.y_actual, y_pred=self.y_predicted)\n",
    "        else:\n",
    "            self.y_actual = None\n",
    "            self.y_predicted = None\n",
    "            self.error = info_dict['error']\n",
    "\n",
    "    def display_y_comparison(self, transpose=True):\n",
    "        y_comp = pd.DataFrame({'actual': list(self.y_actual), 'predicted': self.y_predicted})\n",
    "        if transpose:\n",
    "            y_comp = y_comp.transpose()\n",
    "        return display(y_comp)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_error(y_act, y_pred):\n",
    "        def calculate_mae():\n",
    "            test_error = 0\n",
    "            for actual, predicted in zip(y_act, y_pred):\n",
    "                test_error += abs(actual - predicted)\n",
    "            return test_error / len(y_pred)\n",
    "\n",
    "        def calculate_mse():\n",
    "            test_error = 0\n",
    "            for actual, predicted in zip(y_act, y_pred):\n",
    "                test_error += (actual - predicted) ** 2\n",
    "            return test_error / len(y_pred)\n",
    "\n",
    "        if ecm == 'mean_absolute_error':\n",
    "            return calculate_mae()\n",
    "        elif ecm == 'mean_squared_error':\n",
    "            return calculate_mse()\n",
    "        else:\n",
    "            print(\"You can't reach here.\")\n",
    "\n",
    "\n",
    "class ModelResultsTable:\n",
    "    def __init__(self):\n",
    "        self.all_model_results = {}  # {'mul_lin_reg': [ModelResult0, ModelResult1, ...], 'lin_reg': [...] ... }\n",
    "        self.model_results_with_best_params = []\n",
    "\n",
    "    def add_model(self, model_results: list):\n",
    "        sorted_models = list(sorted(model_results, key=lambda item: item.error))\n",
    "        best_performing_var = sorted_models[0]\n",
    "        self.all_model_results[best_performing_var.model] = sorted_models\n",
    "        self.model_results_with_best_params.append(best_performing_var)\n",
    "        self.model_results_with_best_params = list(sorted(self.model_results_with_best_params,\n",
    "                                                          key=lambda item: item.error))\n",
    "\n",
    "    def display_error_table(self, transpose=True):\n",
    "        error_table = pd.DataFrame([[result.model, result.arg, result.predictors, result.error]\n",
    "                                    for result in self.model_results_with_best_params],\n",
    "                                   columns=['model', 'args', 'predictors', ecm])\n",
    "        if transpose:\n",
    "            error_table = error_table.transpose()\n",
    "        display(error_table)\n",
    "\n",
    "\n",
    "all_results_table = ModelResultsTable()\n",
    "\n",
    "\n",
    "# Utility Functions\n",
    "def k_fold_sampling(sample_source=dataset):\n",
    "    kf = KFold(n_splits=k_fold_value, random_state=None, shuffle=False)\n",
    "\n",
    "    features = np.array(sample_source[predictors])\n",
    "    labels = np.array(sample_source['Y'])\n",
    "    k_s = []\n",
    "    for train_i, test_i in kf.split(dataset):\n",
    "        k_s.append({'train_features': pd.DataFrame(features[train_i], columns=predictors),\n",
    "                    'train_labels': labels[train_i],\n",
    "                    'test_features': pd.DataFrame(features[test_i], columns=predictors),\n",
    "                    'test_labels': labels[test_i]})\n",
    "    return k_s\n",
    "\n",
    "\n",
    "def k_fold_cv(samples, reg_func, arg=None, inputs=predictors):\n",
    "    k_fold_cv_mae = 0\n",
    "    first_fold_prediction = None\n",
    "    if arg is not None:\n",
    "        arg_key, arg_value = arg[0], arg[1]\n",
    "        arg_text = f'{arg_key}={arg_value}'\n",
    "    else:\n",
    "        arg_value = None\n",
    "        arg_text = ''\n",
    "    if type(inputs) != list:\n",
    "        inputs = [inputs]\n",
    "    for i in range(len(samples)):\n",
    "        x_train = pd.DataFrame(samples[i]['train_features'][inputs], columns=inputs)\n",
    "        x_test = pd.DataFrame(samples[i]['test_features'][inputs], columns=inputs)\n",
    "        ki_result = reg_func(x_train=x_train, y_train=samples[i]['train_labels'],\n",
    "                             x_test=x_test, y_test=samples[i]['test_labels'],\n",
    "                             arg=arg_value, inputs=inputs)\n",
    "        if first_fold_prediction is None:\n",
    "            first_fold_prediction = ki_result.y_predicted\n",
    "        k_fold_cv_mae += ki_result.error\n",
    "\n",
    "    if reg_func == single_linear_regression:\n",
    "        return ModelResult({'model': reg_func.__name__,\n",
    "                            'arg': arg_text,\n",
    "                            'predictors': inputs,\n",
    "                            'error': k_fold_cv_mae / len(samples)}), first_fold_prediction\n",
    "    return ModelResult({'model': reg_func.__name__,\n",
    "                        'arg': arg_text,\n",
    "                        'predictors': inputs,\n",
    "                        'error': k_fold_cv_mae / len(samples)})\n",
    "\n",
    "\n",
    "# Regression Functions\n",
    "def regression(x_train, y_train, x_test):\n",
    "    x_regressor = LinearRegression().fit(x_train, y_train)\n",
    "    return x_regressor.predict(x_test)\n",
    "\n",
    "\n",
    "def single_linear_regression(x_train, y_train, x_test, y_test, arg=None, inputs=None):\n",
    "    y_predicted = regression(x_train=np.array(x_train), y_train=y_train,\n",
    "                             x_test=np.array(x_test))\n",
    "    # y_predicted = regression(x_train=np.array(x_train[inputs])[:, None], y_train=y_train,\n",
    "    #                         x_test=np.array(x_test[inputs])[:, None])\n",
    "    return ModelResult({'model': 'single_linear_regression',\n",
    "                        'arg': arg,\n",
    "                        'predictors': inputs,\n",
    "                        'y_actual': y_test,\n",
    "                        'y_predicted': y_predicted})\n",
    "\n",
    "\n",
    "def multiple_linear_regression(x_train, y_train, x_test, y_test, arg, inputs=predictors):\n",
    "    y_predicted = regression(x_train=x_train[inputs], y_train=y_train, x_test=x_test[inputs])\n",
    "    return ModelResult({'model': 'multiple_linear_regression',\n",
    "                        'arg': arg,\n",
    "                        'predictors': inputs,\n",
    "                        'y_actual': y_test,\n",
    "                        'y_predicted': y_predicted})\n",
    "\n",
    "\n",
    "def polynomial_regression(x_train, y_train, x_test, y_test, arg=2, inputs=predictors):\n",
    "    poly_features = PolynomialFeatures(degree=arg, include_bias=False)\n",
    "    y_predicted = regression(x_train=poly_features.fit_transform(x_train[inputs]), y_train=y_train,\n",
    "                             x_test=poly_features.fit_transform(x_test[inputs]))\n",
    "    return ModelResult({'model': 'polynomial_regression',\n",
    "                        'arg': f'o={arg}',\n",
    "                        'predictors': ', '.join(inputs),\n",
    "                        'y_actual': y_test,\n",
    "                        'y_predicted': y_predicted})\n",
    "\n",
    "\n",
    "def k_nearest_regression(x_train, y_train, x_test, y_test, arg=3, inputs=predictors):\n",
    "    def euclidean_dist(p_x, p_y):\n",
    "        distances = list(map(lambda pr: (p_x[pr] - p_y[pr]) ** 2, predictors))\n",
    "        return sum(distances)\n",
    "\n",
    "    y_predicted = []\n",
    "    for i in range(len(x_test.index)):\n",
    "        current_row = x_test.iloc[i]\n",
    "        current_distances = [[y_train[n], euclidean_dist(current_row, x_train.iloc[n])]\n",
    "                             for n in range(len(x_train.index))]\n",
    "        current_distances.sort(key=lambda n: n[1])\n",
    "        k_nearest_avg = sum([n[0] for n in current_distances[:arg]]) // arg\n",
    "        y_predicted.append(k_nearest_avg)\n",
    "\n",
    "    return ModelResult({'model': 'k_nearest_regression',\n",
    "                        'arg': f'k={arg}',\n",
    "                        'predictors': ', '.join(inputs),\n",
    "                        'y_actual': y_test,\n",
    "                        'y_predicted': y_predicted})\n",
    "\n",
    "\n",
    "def decision_tree_regression(x_train, y_train, x_test, y_test, arg=5, inputs=predictors):\n",
    "    regressor = DecisionTreeRegressor(random_state=0, max_depth=arg).fit(x_train, y_train)\n",
    "    y_predicted = regressor.predict(x_test)\n",
    "\n",
    "    return ModelResult({'model': 'decision_tree_regression',\n",
    "                        'arg': f'd={arg}',\n",
    "                        'predictors': ', '.join(inputs),\n",
    "                        'y_actual': y_test,\n",
    "                        'y_predicted': y_predicted})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before applying any machine learning algorithms, I wanted to see how each of the given predictors relate to each other.\n",
    "To do it, I have found out pairplot() function from seaborn library which takes the name of columns (predictors) as\n",
    "parameters and plots each one of them with another."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.pairplot(dataset[predictors], diag_kind='kde')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From looking at those results, it is clear that x1 and x6 values are exactly the same. So, I have decided to drop x6\n",
    "predictor since we do not need the same values twice. Also, I have previously included SampleNo as a predictor (even it is\n",
    "not really logical) just to see if it makes the algorithms perform better, which it obviously did not. So, I am also\n",
    "dropping the SampleNo from the dataset and the predictors list."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = dataset.drop('x6', axis=1)\n",
    "predictors.remove('x6')\n",
    "\n",
    "dataset = dataset.drop('SampleNo', axis=1)\n",
    "predictors.remove('SampleNo')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, I wanted to see how each of those predictors relate to Y separately."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig_, axes_ = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes_[1][2].set_visible(False)\n",
    "axes_[1][0].set_position([0.24, 0.125, 0.228, 0.343])\n",
    "axes_[1][1].set_position([0.55, 0.125, 0.228, 0.343])\n",
    "\n",
    "for i_ in range(len(predictors)):\n",
    "    sns.scatterplot(ax=axes_[i_ // 3, i_ % 3], x=dataset[predictors[i_]], y=dataset['Y'], alpha=0.5, color='b')\n",
    "\n",
    "table_heuristics = dataset.describe().transpose()[['min', 'max', 'mean', 'std']]\n",
    "display(table_heuristics)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From here, I can see that when x1 has low values, their corresponding Y values are also tend to be lower. Similarly,\n",
    "when x5 has a value greater than 5, its corresponding Y value is lower. Because of this, I assume that some kind of\n",
    "classification method must be applied."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, normalize\n",
    "#\n",
    "# scaled_data = StandardScaler().fit_transform(dataset[predictors])\n",
    "# x_normalized = normalize(scaled_data, norm='l1', axis=0)\n",
    "# dataset_normalized = pd.DataFrame(x_normalized, columns=predictors)\n",
    "#\n",
    "# dataset_normalized['Y'] = dataset['Y']\n",
    "# dataset = dataset_normalized\n",
    "\n",
    "k_samples = k_fold_sampling(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig_, axes_ = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes_[1][2].set_visible(False)\n",
    "axes_[1][0].set_position([0.24, 0.125, 0.228, 0.343])\n",
    "axes_[1][1].set_position([0.55, 0.125, 0.228, 0.343])\n",
    "\n",
    "for i_ in range(len(predictors)):\n",
    "    sns.scatterplot(ax=axes_[i_ // 3, i_ % 3], x=dataset[predictors[i_]], y=dataset['Y'], alpha=0.5, color='b')\n",
    "\n",
    "table_heuristics = dataset.describe().transpose()[['min', 'max', 'mean', 'std']]\n",
    "display(table_heuristics)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# I. Linear Regression\n",
    "\n",
    "## I.I. Single Linear Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes[1][2].set_visible(False)\n",
    "axes[1][0].set_position([0.24, 0.125, 0.228, 0.343])\n",
    "axes[1][1].set_position([0.55, 0.125, 0.228, 0.343])\n",
    "\n",
    "# lin_error_dict = {}\n",
    "lin_reg_results = []\n",
    "ax_i = 0\n",
    "for p in predictors:\n",
    "    ax = axes[ax_i // 3, ax_i % 3]\n",
    "\n",
    "    k_fold_result, first_fold_pred = k_fold_cv(k_samples, single_linear_regression, inputs=p)\n",
    "    lin_reg_results.append(k_fold_result)\n",
    "\n",
    "    sns.scatterplot(ax=ax, x=k_samples[0]['test_features'][p], y=k_samples[0]['test_labels'],\n",
    "                    alpha=0.7, color='b', label='Test Data')\n",
    "    sns.scatterplot(ax=ax, x=k_samples[0]['train_features'][p], y=k_samples[0]['train_labels'],\n",
    "                    alpha=0.3, color='g', label='Training Data')\n",
    "    sns.regplot(ax=ax, x=k_samples[0]['test_features'][p], y=first_fold_pred,\n",
    "                color='r', scatter=False, label='Predicted Y', truncate=False)\n",
    "\n",
    "    ax.set_xlim(table_heuristics['min'][p], table_heuristics['max'][p])\n",
    "    ax.set_xlabel(p)\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.legend()\n",
    "    ax_i += 1\n",
    "\n",
    "all_results_table.add_model(lin_reg_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## I.II. Multiple Linear Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "predictor_combinations = []\n",
    "for L in range(2, len(predictors)+1):\n",
    "    predictor_combinations.extend(itertools.combinations(predictors, L))\n",
    "\n",
    "multi_lin_reg_args = list(map(list, predictor_combinations))\n",
    "multi_lin_reg_results = [k_fold_cv(samples=k_samples, reg_func=multiple_linear_regression, inputs=ps)\n",
    "                         for ps in multi_lin_reg_args]\n",
    "\n",
    "all_results_table.add_model(multi_lin_reg_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## I.III. Polynomial Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "poly_reg_args = [2, 3, 4]\n",
    "poly_reg_results = [k_fold_cv(samples=k_samples, reg_func=polynomial_regression, arg=['o', order])\n",
    "                    for order in poly_reg_args]\n",
    "\n",
    "all_results_table.add_model(poly_reg_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## I.IV. K-Nearest-Neighbors Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn_args = [2, 3, 4, 5]\n",
    "k_nearest_results = [k_fold_cv(samples=k_samples, reg_func=k_nearest_regression, arg=['k', k]) for k in knn_args]\n",
    "\n",
    "all_results_table.add_model(k_nearest_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## I.V. Decision Tree Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dec_tree_args = range(1, 11)\n",
    "dec_tree_results = [k_fold_cv(samples=k_samples, reg_func=decision_tree_regression, arg=['d', d])\n",
    "                    for d in dec_tree_args]\n",
    "\n",
    "all_results_table.add_model(dec_tree_results)\n",
    "all_results_table.display_error_table(transpose=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TESTING"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def random_forest_regression(x_train, y_train, x_test, y_test, arg=None, inputs=predictors):\n",
    "    if arg is None:\n",
    "        arg = ['d', 2]\n",
    "    regressor = RandomForestRegressor(max_depth=arg, random_state=0).fit(x_train, y_train)\n",
    "    y_predicted = regressor.predict(x_test)\n",
    "\n",
    "    return ModelResult({'model': 'random_forest_regression',\n",
    "                        'arg': f'd={arg}',\n",
    "                        'predictors': ', '.join(inputs),\n",
    "                        'y_actual': y_test,\n",
    "                        'y_predicted': y_predicted})\n",
    "\n",
    "rand_forest_args = range(1, 11)\n",
    "rand_forest_results = [k_fold_cv(samples=k_samples, reg_func=random_forest_regression,\n",
    "                                 arg=['d', d], inputs=predictors) for d in rand_forest_args]\n",
    "all_results_table.add_model(rand_forest_results)\n",
    "all_results_table.display_error_table(transpose=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -v -m -p sklearn,pandas,numpy,matplotlib,seaborn\n",
    "%watermark -u -n -t -z"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}