{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports and Configurations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "ecm = 'mean_absolute_error'\n",
    "# ecm = 'mean_squared_error'\n",
    "k_fold_value = 5\n",
    "\n",
    "raw_dataset = pd.read_csv('dataset_v1.csv', sep=',', skipinitialspace=True)\n",
    "dataset = raw_dataset.copy()\n",
    "predict_set = dataset.tail(20)\n",
    "dataset = dataset.drop(predict_set.index)\n",
    "\n",
    "predictors = ['SampleNo', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6']\n",
    "all_test_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def random_sampling(train_fraction=0.8):\n",
    "    train = dataset.sample(frac=train_fraction, random_state=0)\n",
    "    test = dataset.drop(train.index)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def k_fold_sampling(sample_source=dataset):\n",
    "    kf = KFold(n_splits=k_fold_value, random_state=None, shuffle=False)\n",
    "\n",
    "    features = np.array(sample_source[predictors])\n",
    "    labels = np.array(sample_source['Y'])\n",
    "    k_s = []\n",
    "    for train_i, test_i in kf.split(dataset):\n",
    "        k_s.append({'train_features': pd.DataFrame(features[train_i], columns=predictors),\n",
    "                    'train_labels': labels[train_i],\n",
    "                    'test_features': pd.DataFrame(features[test_i], columns=predictors),\n",
    "                    'test_labels': labels[test_i]})\n",
    "    return k_s\n",
    "\n",
    "\n",
    "def k_fold_cv(samples, reg_func, arg=None):\n",
    "    k_fold_cv_mae = 0\n",
    "    k_fold_comparisons = []\n",
    "    model_name = None\n",
    "    for i in range(len(samples)):\n",
    "        ki_result = reg_func(x_train=samples[i]['train_features'], y_train=samples[i]['train_labels'],\n",
    "                             x_test=samples[i]['test_features'], y_test=samples[i]['test_labels'], arg=arg)\n",
    "        model_name = ki_result['model']\n",
    "        k_fold_comparisons.append(ki_result['comparison'])\n",
    "        k_fold_cv_mae += ki_result['error']\n",
    "\n",
    "    return {'model': model_name,\n",
    "            'comparison': k_fold_comparisons,\n",
    "            'error': k_fold_cv_mae / len(k_samples)}\n",
    "\n",
    "\n",
    "def calculate_error(y_act, y_pred):\n",
    "    def calculate_mae():\n",
    "        test_error = 0\n",
    "        for actual, predicted in zip(y_act, y_pred):\n",
    "            test_error += abs(actual - predicted)\n",
    "        return test_error / len(y_pred)\n",
    "\n",
    "    def calculate_mse():\n",
    "        test_error = 0\n",
    "        for actual, predicted in zip(y_act, y_pred):\n",
    "            test_error += (actual - predicted) ** 2\n",
    "        return test_error / len(y_pred)\n",
    "\n",
    "    if ecm == 'mean_absolute_error':\n",
    "        return calculate_mae()\n",
    "    elif ecm == 'mean_squared_error':\n",
    "        return calculate_mse()\n",
    "    else:\n",
    "        print(\"You can't reach here.\")\n",
    "\n",
    "\n",
    "def display_error_dict(error_dict, transpose=True):\n",
    "    error_table = pd.DataFrame([[k, v] for k, v in error_dict.items()], columns=['model', ecm])\n",
    "    if transpose:\n",
    "        error_table = error_table.transpose()\n",
    "    display(error_table)\n",
    "\n",
    "\n",
    "def sort_error_dict(result_dict):\n",
    "    return dict(sorted(result_dict.items(), key=lambda item: item[1]))\n",
    "\n",
    "\n",
    "class ModelResult:\n",
    "    def __init__(self, info_dict: dict):\n",
    "        self.model = info_dict['model']\n",
    "        self.arg = info_dict['arg']\n",
    "        self.predictors = info_dict['predictors']\n",
    "        self.y_actual = info_dict['y_actual']\n",
    "        self.y_predicted = info_dict['y_predicted']\n",
    "        self.error = calculate_error(y_act=self.y_actual, y_pred=self.y_predicted)\n",
    "\n",
    "    def display_y_comparison(self, transpose=True):\n",
    "        y_comp = pd.DataFrame({'actual': list(self.y_actual), 'predicted': self.y_predicted})\n",
    "        if transpose:\n",
    "            y_comp = y_comp.transpose()\n",
    "        return display(y_comp)\n",
    "\n",
    "\n",
    "class ModelResultsTable:\n",
    "    def __init__(self):\n",
    "        # ModelResult({'model': 'mul_lin_reg', 'arg': 'o=2', 'predictors': ['x1', 'x3'] ... })\n",
    "        self.all_model_results = {}  # {'mul_lin_reg': [ModelResult0, ModelResult1, ...], 'lin_reg': [...] ... }\n",
    "        self.model_results_with_best_params = []\n",
    "\n",
    "    def add_model(self, model_results: list):\n",
    "        sorted_models = list(sorted(model_results, key=lambda item: item.error))\n",
    "        best_performing_var = sorted_models[0]\n",
    "        self.all_model_results[best_performing_var.model] = sorted_models\n",
    "        self.model_results_with_best_params = list(\n",
    "            sorted(self.model_results_with_best_params.append(best_performing_var),\n",
    "                   key=lambda item: item.error))\n",
    "\n",
    "    def display_error_table(self, transpose=True):\n",
    "        error_table = pd.DataFrame([[result.model, result.arg, result.predictors, result.error]\n",
    "                                    for result in self.model_results_with_best_params],\n",
    "                                   columns=['model', ecm, 'args', 'predictors'])\n",
    "        if transpose:\n",
    "            error_table = error_table.transpose()\n",
    "        display(error_table)\n",
    "\n",
    "all_results_table = ModelResultsTable()\n",
    "\n",
    "# Regression Functions\n",
    "def regression(model, x_train, y_train, x_test, y_test, results_as_df=True):\n",
    "    x_regressor = LinearRegression().fit(x_train, y_train)\n",
    "    y_predicted = x_regressor.predict(x_test)\n",
    "\n",
    "    y_comparison = {'actual': list(y_test), 'predicted': y_predicted}\n",
    "    mae_error = calculate_error(y_act=y_comparison['actual'], y_pred=y_comparison['predicted'])\n",
    "\n",
    "    if results_as_df:\n",
    "        y_comparison = pd.DataFrame(y_comparison).transpose()\n",
    "\n",
    "    return {'model': model,\n",
    "            'comparison': y_comparison,\n",
    "            'error': mae_error}\n",
    "\n",
    "\n",
    "def single_linear_regression(x_train, y_train, x_test, y_test, arg):\n",
    "    return regression(model=f'linear_reg_{arg}',\n",
    "                      x_train=np.array(x_train[arg])[:, None], y_train=y_train,\n",
    "                      x_test=np.array(x_test[arg])[:, None], y_test=y_test)\n",
    "\n",
    "\n",
    "def multiple_linear_regression(x_train, y_train, x_test, y_test, arg=predictors):\n",
    "    return regression(model=f'mul_linear_reg_{arg}',\n",
    "                      x_train=x_train[arg], y_train=y_train,\n",
    "                      x_test=x_test[arg], y_test=y_test)\n",
    "\n",
    "\n",
    "def polynomial_regression(x_train, y_train, x_test, y_test, arg=2):\n",
    "    poly_features = PolynomialFeatures(degree=arg, include_bias=False)\n",
    "    return regression(model=f'polynomial_reg_o={arg}',\n",
    "                      x_train=poly_features.fit_transform(x_train), y_train=y_train,\n",
    "                      x_test=poly_features.fit_transform(x_test), y_test=y_test)\n",
    "\n",
    "\n",
    "def k_nearest_regression(x_train, y_train, x_test, y_test, arg=3):\n",
    "    def euclidean_dist(p_x, p_y):\n",
    "        distances = list(map(lambda pr: (p_x[pr] - p_y[pr]) ** 2, predictors))\n",
    "        return sum(distances)\n",
    "\n",
    "    predictions = []\n",
    "    for i in range(len(x_test.index)):\n",
    "        current_row = x_test.iloc[i]\n",
    "        current_distances = [[y_train[n], euclidean_dist(current_row, x_train.iloc[n])]\n",
    "                             for n in range(len(x_train.index))]\n",
    "        current_distances.sort(key=lambda n: n[1])\n",
    "        k_nearest_avg = sum([n[0] for n in current_distances[:arg]]) // arg\n",
    "        predictions.append(k_nearest_avg)\n",
    "\n",
    "    y_comparison = {'actual': list(y_test), 'predicted': predictions}\n",
    "    reg_error = calculate_error(y_act=y_comparison['actual'], y_pred=y_comparison['predicted'])\n",
    "\n",
    "    return {'model': f'k_nearest_reg_k={arg}',\n",
    "            'comparison': pd.DataFrame(y_comparison).transpose(),\n",
    "            'error': reg_error}\n",
    "\n",
    "\n",
    "def decision_tree_regression(x_train, y_train, x_test, y_test, arg=5):\n",
    "    regressor = DecisionTreeRegressor(random_state=0, max_depth=arg).fit(x_train, y_train)\n",
    "    y_predicted = regressor.predict(x_test)\n",
    "\n",
    "    y_comparison = {'actual': list(y_test), 'predicted': y_predicted}\n",
    "    mae_error = calculate_error(y_act=y_comparison['actual'], y_pred=y_comparison['predicted'])\n",
    "\n",
    "    return {'model': f'decision_tree_reg_d={arg}',\n",
    "            'comparison': y_comparison,\n",
    "            'error': mae_error}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before applying any machine learning algorithms, I wanted to see how each of the given predictors relate to each other.\n",
    "To do it, I have found out pairplot() function from seaborn library which takes the name of columns (predictors) as\n",
    "parameters and plots each one of them with another."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.pairplot(dataset[predictors], diag_kind='kde')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From looking at those results, it is clear that x1 and x6 values are exactly the same. So, I have decided to drop x6\n",
    "predictor since we do not need the same values twice. Also, I have previously included SampleNo as a predictor (even it is\n",
    "not really logical) just to see if it makes the algorithms perform better, which it obviously did not. So, I am also\n",
    "dropping the SampleNo from the dataset and the predictors list."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = dataset.drop('x6', axis=1)\n",
    "predictors.remove('x6')\n",
    "\n",
    "dataset = dataset.drop('SampleNo', axis=1)\n",
    "predictors.remove('SampleNo')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, I wanted to see how each of those predictors relate to Y separately."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig_, axes_ = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes_[1][2].set_visible(False)\n",
    "axes_[1][0].set_position([0.24, 0.125, 0.228, 0.343])\n",
    "axes_[1][1].set_position([0.55, 0.125, 0.228, 0.343])\n",
    "\n",
    "for i_ in range(len(predictors)):\n",
    "    sns.scatterplot(ax=axes_[i_ // 3, i_ % 3], x=dataset[predictors[i_]], y=dataset['Y'], alpha=0.5, color='b')\n",
    "\n",
    "table_heuristics = dataset.describe().transpose()[['min', 'max', 'mean', 'std']]\n",
    "display(table_heuristics)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From here, I can see that when x1 has low values, their corresponding Y values are also tend to be lower. Similarly,\n",
    "when x5 has a value greater than 5, its corresponding Y value is lower. Because of this, I assume that some kind of\n",
    "classification method must be applied."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "\n",
    "scaled_data = StandardScaler().fit_transform(dataset[predictors])\n",
    "x_normalized = normalize(scaled_data, norm='l1', axis=1)\n",
    "dataset_normalized = pd.DataFrame(x_normalized, columns=predictors)\n",
    "\n",
    "dataset_normalized['Y'] = dataset['Y']\n",
    "dataset = dataset_normalized\n",
    "k_samples = k_fold_sampling(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig_, axes_ = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes_[1][2].set_visible(False)\n",
    "axes_[1][0].set_position([0.24, 0.125, 0.228, 0.343])\n",
    "axes_[1][1].set_position([0.55, 0.125, 0.228, 0.343])\n",
    "\n",
    "for i_ in range(len(predictors)):\n",
    "    sns.scatterplot(ax=axes_[i_ // 3, i_ % 3], x=dataset[predictors[i_]], y=dataset['Y'], alpha=0.5, color='b')\n",
    "\n",
    "table_heuristics = dataset.describe().transpose()[['min', 'max', 'mean', 'std']]\n",
    "display(table_heuristics)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# I. Linear Regression\n",
    "\n",
    "## I.I. Single Linear Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes[1][2].set_visible(False)\n",
    "axes[1][0].set_position([0.24, 0.125, 0.228, 0.343])\n",
    "axes[1][1].set_position([0.55, 0.125, 0.228, 0.343])\n",
    "\n",
    "lin_error_dict = {}\n",
    "ax_i = 0\n",
    "for p in predictors:\n",
    "    ax = axes[ax_i // 3, ax_i % 3]\n",
    "\n",
    "    k_fold_result = k_fold_cv(k_samples, single_linear_regression, arg=p)\n",
    "    lin_error_dict[k_fold_result['model']] = k_fold_result['error']\n",
    "\n",
    "    sns.scatterplot(ax=ax, x=k_samples[0]['test_features'][p], y=k_samples[0]['test_labels'],\n",
    "                    alpha=0.7, color='b', label='Test Data')\n",
    "    sns.scatterplot(ax=ax, x=k_samples[0]['train_features'][p], y=k_samples[0]['train_labels'],\n",
    "                    alpha=0.3, color='g', label='Training Data')\n",
    "    sns.regplot(ax=ax, x=k_samples[0]['test_features'][p], y=k_fold_result['comparison'][0].loc['predicted'],\n",
    "                color='r', scatter=False, label='Predicted Y', truncate=False)\n",
    "\n",
    "    ax.set_xlim(table_heuristics['min'][p], table_heuristics['max'][p])\n",
    "    ax.set_xlabel(p)\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.legend()\n",
    "    ax_i += 1\n",
    "\n",
    "lin_error_dict = sort_error_dict(lin_error_dict)\n",
    "display_error_dict(lin_error_dict)\n",
    "\n",
    "best_performing_model = list(lin_error_dict.items())[0]\n",
    "all_test_results[best_performing_model[0]] = best_performing_model[1]\n",
    "all_test_results = sort_error_dict(all_test_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## I.II. Multiple Linear Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "predictor_combinations = []\n",
    "for L in range(2, len(predictors)+1):\n",
    "    predictor_combinations.extend(itertools.combinations(predictors, L))\n",
    "\n",
    "multi_lin_reg_args = list(map(list, predictor_combinations))\n",
    "multi_lin_reg_results = [k_fold_cv(samples=k_samples, reg_func=multiple_linear_regression, arg=ps)\n",
    "                         for ps in multi_lin_reg_args]\n",
    "\n",
    "multi_lin_error_dict = {}\n",
    "for multi_lin_reg_result in multi_lin_reg_results:\n",
    "    multi_lin_error_dict[multi_lin_reg_result['model']] = multi_lin_reg_result['error']\n",
    "multi_lin_error_dict = sort_error_dict(multi_lin_error_dict)\n",
    "display_error_dict(multi_lin_error_dict)\n",
    "\n",
    "best_performing_model = list(multi_lin_error_dict.items())[0]\n",
    "all_test_results[best_performing_model[0]] = best_performing_model[1]\n",
    "all_test_results = sort_error_dict(all_test_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## I.III. Polynomial Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "poly_reg_args = [2, 3, 4]\n",
    "poly_reg_results = [k_fold_cv(samples=k_samples, reg_func=polynomial_regression, arg=order) for order in poly_reg_args]\n",
    "\n",
    "poly_error_dict = {}\n",
    "for poly_reg_result in poly_reg_results:\n",
    "    poly_error_dict[poly_reg_result['model']] = poly_reg_result['error']\n",
    "poly_error_dict = sort_error_dict(poly_error_dict)\n",
    "display_error_dict(poly_error_dict)\n",
    "\n",
    "best_performing_model = list(poly_error_dict.items())[0]\n",
    "all_test_results[best_performing_model[0]] = best_performing_model[1]\n",
    "all_test_results = sort_error_dict(all_test_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## I.IV. K-Nearest-Neighbors Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn_args = [2, 3, 4, 5]\n",
    "\n",
    "k_nearest_results = [k_fold_cv(samples=k_samples, reg_func=k_nearest_regression, arg=k) for k in knn_args]\n",
    "\n",
    "knn_error_dict = {}\n",
    "for knn_result in k_nearest_results:\n",
    "    knn_error_dict[knn_result['model']] = knn_result['error']\n",
    "knn_error_dict = sort_error_dict(knn_error_dict)\n",
    "display_error_dict(knn_error_dict)\n",
    "\n",
    "best_performing_model = list(knn_error_dict.items())[0]\n",
    "all_test_results[best_performing_model[0]] = best_performing_model[1]\n",
    "all_test_results = sort_error_dict(all_test_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TESTING"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## I.V. Decision Tree Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dec_tree_args = range(2, 10)\n",
    "dec_tree_results = [k_fold_cv(samples=k_samples, reg_func=decision_tree_regression, arg=d) for d in dec_tree_args]\n",
    "\n",
    "dec_tree_error_dict = {}\n",
    "for dec_tree_result in dec_tree_results:\n",
    "    dec_tree_error_dict[dec_tree_result['model']] = dec_tree_result['error']\n",
    "dec_tree_error_dict = sort_error_dict(dec_tree_error_dict)\n",
    "display_error_dict(dec_tree_error_dict)\n",
    "\n",
    "best_performing_model = list(dec_tree_error_dict.items())[0]\n",
    "all_test_results[best_performing_model[0]] = best_performing_model[1]\n",
    "all_test_results = sort_error_dict(all_test_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display_error_dict(all_test_results, transpose=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -v -m -p sklearn,pandas,numpy,matplotlib,seaborn\n",
    "%watermark -u -n -t -z"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}